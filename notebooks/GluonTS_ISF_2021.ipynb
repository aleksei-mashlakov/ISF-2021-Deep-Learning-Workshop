{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Forecasting using GluonTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GluonTS is an open source Python library for time series modeling, with a focus on deep learning architectures for forecasting.\n",
    "It provides:\n",
    "* Abstractions for prediction models\n",
    "* Tools for model evaluation (backtesting)\n",
    "* Model components, to help build custom models\n",
    "* Pre-implemented, state-of-the-art forecasting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We originally developed GluonTS on top of MXNet's Gluon API (hence the name), but the library is now compatible with PyTorch, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "* GitHub: https://github.com/awslabs/gluon-ts (to file issues, trigger discussions, contribute changes, ...)\n",
    "* Website: https://ts.gluon.ai (documentation, tutorials, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target users:\n",
    "* **Researchers** who want to try out novel architectures for forecasting and compare them to the state of the art\n",
    "* **Data scientist/solution architects** who want to experiment with several solutions to solve business problems\n",
    "* **Machine learning engineer** who need to integrate forecasting models in production pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you're going to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Handle datasets in GluonTS (transform them and iterate them)\n",
    "* Forecast and do backtest evaluations given a model (the \"predictor\" interface)\n",
    "* Train a model based on pre-implemented architectures (the \"estimator\" interface)\n",
    "* Implement a custom, neural-network-based architecture using PyTorch, and use it with the rest of GluonTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Dict, Tuple, Any\n",
    "from itertools import islice\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GluonTS comes with a set of publicly available dataset, ready to use for training and testing models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.repository.datasets import get_dataset, dataset_names\n",
    "from gluonts.dataset.util import to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we're going to use the hourly dataset from the M4 competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(\"traffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(next(iter(dataset.train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pandas(next(iter(dataset.train)))[-24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for idx, entry in enumerate(islice(dataset.train, 9)):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "    to_pandas(entry)[-168:].plot()\n",
    "    plt.grid()\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.seasonal_naive import SeasonalNaivePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_naive = SeasonalNaivePredictor(\n",
    "    freq=dataset.metadata.freq,\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    season_length=24 * 7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = list(predictor_naive.predict(dataset.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for idx, (entry, f) in islice(enumerate(zip(dataset.train, forecasts)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "    to_pandas(entry)[-24 * 7:].plot()\n",
    "    f.plot(color=\"r\")\n",
    "    plt.grid()\n",
    "    plt.title(f\"item_id: {f.item_id}\")\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import backtest_metrics, make_evaluation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_metrics_naive, entrywise_metrics_naive = backtest_metrics(dataset.test, predictor_naive, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations = pd.DataFrame.from_records(\n",
    "    [aggregate_metrics_naive],\n",
    "    index=[\"Naive seasonal\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrywise_metrics_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = entrywise_metrics_naive.groupby(\"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb[\"MAPE\"].agg(np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using pre-implemented models: DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch.model.deepar import DeepAREstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = dataset.metadata.prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar_estimator = DeepAREstimator(\n",
    "    freq=dataset.metadata.freq,\n",
    "    prediction_length=prediction_length,\n",
    "    num_feat_static_cat=len(dataset.metadata.feat_static_cat),\n",
    "    cardinality=[int(f.cardinality) for f in dataset.metadata.feat_static_cat],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar_predictor = deepar_estimator.train(dataset.train, cache_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do backtesting on the test dataset: in what follows, `make_evaluation_predictions` will slice out the trailing `prediction_length` observations from the test time series, and use the given predictor to obtain forecasts for the same time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset.test,\n",
    "    predictor=deepar_predictor,\n",
    ")\n",
    "\n",
    "forecasts_deepar = list(forecast_it)\n",
    "tss_deepar = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formatter = matplotlib.dates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts_deepar, tss_deepar)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "    \n",
    "    plt.plot(ts[-5 * prediction_length:], label=\"target\")\n",
    "    forecast.plot(color=\"g\")\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formatter)\n",
    "    \n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_metrics_deepar, entrywise_metrics_deepar = evaluator(tss_deepar, forecasts_deepar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(\n",
    "    [aggregate_metrics_naive, aggregate_metrics_deepar],\n",
    "    index=[\"Naive seasonal\", \"DeepAR\"]\n",
    ").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing a PyTorch model: a probabilistic feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pretty simple model, based on a feed-forward network whose output layer produces the parameters of a Student's t-distribution at each time step in the prediction range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/feedforward.png\" style=\"width: 300px; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch.modules.distribution_output import StudentTOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_scaling(seq: torch.Tensor, min_scale=1e-5):\n",
    "    return seq.abs().mean(1).clamp(min_scale, None).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq: str,\n",
    "        prediction_length: int,\n",
    "        context_length: int,\n",
    "        hidden_dimensions: List[int],\n",
    "        distr_output=StudentTOutput(),\n",
    "        batch_norm: bool = False,\n",
    "        scaling: Callable = mean_abs_scaling,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert prediction_length > 0\n",
    "        assert context_length > 0\n",
    "        assert len(hidden_dimensions) > 0\n",
    "\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length\n",
    "        self.hidden_dimensions = hidden_dimensions\n",
    "        self.distr_output = distr_output\n",
    "        self.batch_norm = batch_norm\n",
    "        self.scaling = scaling\n",
    "\n",
    "        dimensions = [2 * context_length] + hidden_dimensions[:-1]\n",
    "\n",
    "        modules = []\n",
    "        for in_size, out_size in zip(dimensions[:-1], dimensions[1:]):\n",
    "            modules += [self.__make_lin(in_size, out_size), nn.ReLU()]\n",
    "            if batch_norm:\n",
    "                modules.append(nn.BatchNorm1d(out_size))\n",
    "        modules.append(\n",
    "            self.__make_lin(\n",
    "                dimensions[-1], prediction_length * hidden_dimensions[-1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.nn = nn.Sequential(*modules)\n",
    "        self.args_proj = self.distr_output.get_args_proj(hidden_dimensions[-1])\n",
    "\n",
    "    @staticmethod\n",
    "    def __make_lin(dim_in, dim_out):\n",
    "        lin = nn.Linear(dim_in, dim_out)\n",
    "        torch.nn.init.uniform_(lin.weight, -0.07, 0.07)\n",
    "        torch.nn.init.zeros_(lin.bias)\n",
    "        return lin\n",
    "\n",
    "    def forward(self, context: torch.Tensor, context_observed: torch.Tensor) -> Tuple[Tuple[torch.Tensor, ...], torch.Tensor, torch.Tensor]:\n",
    "        assert context.shape == context_observed.shape\n",
    "        \n",
    "        scale = self.scaling(context)\n",
    "        scaled_context = context / scale\n",
    "        nn_in = torch.cat((scaled_context, context_observed), dim=-1)\n",
    "        nn_out = self.nn(nn_in)\n",
    "        nn_out_reshaped = nn_out.reshape(\n",
    "            -1, self.prediction_length, self.hidden_dimensions[-1]\n",
    "        )\n",
    "        distr_args = self.args_proj(nn_out_reshaped)\n",
    "\n",
    "        return distr_args, torch.zeros_like(scale), scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: all inputs and outputs of the `forward` method will have shape `batch_size x context_length`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate the training network, and explore its set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = dataset.metadata.freq\n",
    "prediction_length = dataset.metadata.prediction_length\n",
    "context_length = 2 * 7 * 24\n",
    "hidden_dimensions = [96, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForwardModel(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    hidden_dimensions=hidden_dimensions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.prod(p.shape) for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must specify how to *train* the model, i.e. how the loss will be computed to perform training via SGD-like algorithms. One way to do this, in the PyTorch ecosystem, is to wrap the model within a `LightningModule` from the PyTorch Lightning library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model: FeedForwardModel):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch: Dict[str, Any], batch_idx: int):\n",
    "        context = batch[\"past_target\"]\n",
    "        context_observed = batch[\"past_observed_values\"]\n",
    "        target = batch[\"future_target\"]\n",
    "\n",
    "        assert context.shape[-1] == self.model.context_length\n",
    "        assert target.shape[-1] == self.model.prediction_length\n",
    "\n",
    "        distr_args, loc, scale = self.model(context, context_observed)\n",
    "        distr = self.model.distr_output.distribution(distr_args, loc, scale)\n",
    "        loss = -distr.log_prob(target)\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_module = FeedForwardLightningModule(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the data loader which will yield batches of data to train on. Starting from the original dataset, the data loader is configured to apply the following transformation, which does essentially two things:\n",
    "* Replaces `nan`s in the target field with a dummy value (zero), and adds a field indicating which values were actually observed vs imputed this way.\n",
    "* Slices out training instances of a fixed length randomly from the given dataset; these will be stacked into batches by the data loader itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.transform import Chain, AddObservedValuesIndicator, InstanceSplitter, ExpectedNumInstanceSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = AddObservedValuesIndicator(\n",
    "    target_field=\"target\",\n",
    "    output_field=\"observed_values\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = transformation.apply(dataset.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(next(iter(transformed_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.transform import InstanceSplitter, SelectFields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/training_data_iter.png\" style=\"width: 800px; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = InstanceSplitter(\n",
    "    start_field=\"start\",\n",
    "    target_field=\"target\",\n",
    "    is_pad_field=\"is_pad\",\n",
    "    forecast_start_field=\"forecast_start\",\n",
    "    instance_sampler=ExpectedNumInstanceSampler(\n",
    "        num_instances=1,\n",
    "        min_future=prediction_length,\n",
    "    ),\n",
    "    past_length=context_length,\n",
    "    future_length=prediction_length,\n",
    "    time_series_fields=[\"observed_values\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_fields = SelectFields([\"past_target\", \"past_observed_values\", \"future_target\", \"future_observed_values\", \"item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.itertools import Cyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = (splitter + select_fields).apply(Cyclic(transformed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance in training_instances:\n",
    "    pprint(instance)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.itertools import IterableSlice\n",
    "from gluonts.torch.util import IterableDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batches = IterableSlice(\n",
    "    iter(DataLoader(IterableDataset(training_instances), batch_size=32)),\n",
    "    50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(training_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"past_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"past_target\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"past_observed_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"past_observed_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"future_target\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"future_observed_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the batches in the stream are composed of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = []\n",
    "for epoch_no in range(10):\n",
    "    for batch in training_batches:\n",
    "        batch_ids.append(batch[\"item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_batch = np.zeros(shape=(len(dataset.train), len(batch_ids)))\n",
    "for k, ids in enumerate(batch_ids):\n",
    "    for id in ids:\n",
    "        count_per_batch[id, k] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(count_per_batch)\n",
    "plt.title(\"Training batches\")\n",
    "plt.xlabel(\"Batch no.\")\n",
    "plt.ylabel(\"Item ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model using any of the available optimizers from PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(training_module, train_dataloader=training_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictor out of the trained model, and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch.model.predictor import PyTorchPredictor\n",
    "from gluonts.model.forecast_generator import DistributionForecastGenerator\n",
    "from gluonts.transform import TestSplitSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictor(input_transform, model, batch_size=32):\n",
    "    splitter = InstanceSplitter(\n",
    "        start_field=\"start\",\n",
    "        target_field=\"target\",\n",
    "        is_pad_field=\"is_pad\",\n",
    "        forecast_start_field=\"forecast_start\",\n",
    "        instance_sampler=TestSplitSampler(),\n",
    "        past_length=context_length,\n",
    "        future_length=prediction_length,\n",
    "        time_series_fields=[\"observed_values\"],\n",
    "    )\n",
    "    return PyTorchPredictor(\n",
    "        freq=model.freq,\n",
    "        prediction_length=model.prediction_length,\n",
    "        input_names=[\"past_target\", \"past_observed_values\"],\n",
    "        prediction_net=model,\n",
    "        batch_size=batch_size,\n",
    "        input_transform=input_transform + splitter,\n",
    "        forecast_generator=DistributionForecastGenerator(model.distr_output),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = get_predictor(input_transform=transformation, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/prediction_data_iter.png\" style=\"width: 800px; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset.test,\n",
    "    predictor=predictor,\n",
    "    num_samples=100,\n",
    ")\n",
    "\n",
    "forecasts_feedforward = list(f.to_sample_forecast() for f in forecast_it)\n",
    "tss_feedforward = list(ts_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the forecasts, we can plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formatter = matplotlib.dates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts_feedforward, tss_feedforward)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "    plt.plot(ts[-5 * prediction_length:], label=\"target\")\n",
    "    forecast.plot(color=\"g\")\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formatter)\n",
    "    \n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compute evaluation metrics, that summarize the performance of the model on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_metrics_feedforward, entrywise_metrics_feedforward = evaluator(tss_feedforward, forecasts_feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(\n",
    "    [aggregate_metrics_naive, aggregate_metrics_deepar, aggregate_metrics_feedforward],\n",
    "    index=[\"Naive seasonal\", \"DeepAR\", \"Feed-forward\"]\n",
    ").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several additional features in GluonTS we did not cover here:\n",
    "\n",
    "* Models (mainly implemented in MXNet):\n",
    "    * Wavenet [van den Oord et al., 2016]\n",
    "    * Deep State Space Models [Rangapuram et al., 2018]\n",
    "    * Deep Factors [Wang et al. 2019]\n",
    "    * MQ-RNN/CNN [Wen et al., 2017]\n",
    "    * Attention-based models (transformers)\n",
    "    * Gaussian processes, Temporal point processes, ...\n",
    "    * **Multivariate** models\n",
    "* Serialization/deserialization of models\n",
    "* Helpers to train & deploy models in the cloud, using Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluonts",
   "language": "python",
   "name": "gluonts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
